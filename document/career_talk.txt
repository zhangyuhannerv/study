1.mysql插入数十万条乃至上百万数据时使用mybats-plus的saveBatch()插入速度很慢
执行时间大概3-5min
解决办法：在mysql的连接url上添加rewriteBatchedStatements=true
优化后效率提高10~15倍左右
原理解析：
关于mp的批量插入，底层默认的块大小是1000条，
过程：
先创建一个sqlsession
循环插入的数据。写入批量单条插入语句。insert into t(...) values (...)
当达到1000条时。sqlsession提交
重复该过程
在没有该配置的时候，就是提交多个批量插入的sql
当添加该配置后。mysql的驱动会将多条批量插入语句重写成一条插入语句insert into t(…) values (…) , (…), (…)，然后提交sqlsession

2.mysql大数据量的存储
初始方案：myisam引擎版，简单水平分表
每月月初使用定时任务创建当月的子表（表名：202202），更新主表关联上新键的子表
效果：无论插入还是查询都只需指定主表（相当于一个索引，可以理解为套壳）即可。在插入的时候会插入到最新子表中
查询效率优异。在并发量小的情况下插入速度能接受
演进1：
弃用myisam，改用innodb。不区分主子表。理由：myisam是表锁，不支持事务。innodb是行锁，支持事务
分表改为根据业务建表。即在创建项目（业务里的某条线路）时建一个当前线路的表。表名是项目id。
效果。并发能力提高，插入数据速度提高，支持业务错误回滚，查询效率下降，插入和查询时都需要传入额外表名参数
演进2：
为每个数据表建立复合索引，提升查询效率
演进3：
引入redis缓存机制，提升查询效率
演进4：
分库：
引入了mabatis-plus的多数据源
将大数据量的表单独存放到另一个数据库，与其余的业务表分开
你问我为什么不用mysql主从/集群？因为只有一台服务器





